---
title: "b10b 100m 9.10-SNAPSHOT"
build_number: "2"
dbprofile: mongodb
bench_suite: "17w49 Benchmark 100 Million documents"
date: 2017-12-05T11:55:06
classifier: "171201-1259_3nuxeo-c4.2xlarge_4mongo-m4.xlarge_3es-c4.2xlarge"
type: "custom"
---

## Overview
 
 ![header](https://s3-eu-west-1.amazonaws.com/nuxeo-b10b-results/160617-1739_3mongo-as-c3.2xlarge_1nuxeo-as-c3.2xlarge/b100m-header.png)
 
 The goal is to benchmark a medium volume of 100 million documents on a Nuxeo 9.10-SNAPSHOT and then run the classical Gatling benchmark on it.

 The database is a MongoDB sharded cluster using 4 nodes with a total of 16 CPUs.

 Elasticsearch 5.6.4 is setup with a cluster of 3 nodes with a total of 24 CPUs.

 Kafka 1.0.0 is setup in a single node with 4 CPUs.
 
 Nuxeo 9.10-SNAPSHOT (2017-12-03) in cluster mode with 3 nodes with a total of 24 CPUs.
 
 Nuxeo is tuned to use the StreamWorkManager based on Kafka, Redis is used for dbs invalidation.
 
 The ansible scripts that run this benchmark are available and described here: https://github.com/bdelbosc/b10bpoc/tree/Benchmark-100m-Nuxeo-9.10-SNAP 


## Mass import

  The import is done using 3 Nuxeo nodes. It uses the new nuxeo-importer-stream that generates random source documents and put them into Kafka.
   
  It has taken 1h25min to reach 100m documents, so an average of **19607 docs/s**.
  
  Reporting this to the number of MongoDB vCPU it gives **1225 docs/s per vCPU**. 
  
  The MongoDB storage size is 52.8GiB (including 19.4GiB for the indexes) over 4 nodes, the average document size is 1058 bytes.
   
  ![import steps](https://s3-eu-west-1.amazonaws.com/nuxeo-b10b-results/160617-1739_3mongo-as-c3.2xlarge_1nuxeo-as-c3.2xlarge/b100m-import-steps.png)
  
  ![import rate](https://s3-eu-west-1.amazonaws.com/nuxeo-b10b-results/160617-1739_3mongo-as-c3.2xlarge_1nuxeo-as-c3.2xlarge/b100m-import-rate.png)
   
## Elasticsearch indexing

  Indexing has been run in one shot, during 1:43min, so an average of **15576 docs/s**.
  
  Reporting this to the number of Elasticsearch vCPU it gives **649 docs/s per vCPU**.
   
  ![indexing rate](https://s3-eu-west-1.amazonaws.com/nuxeo-b10b-results/160617-1739_3mongo-as-c3.2xlarge_1nuxeo-as-c3.2xlarge/b100m-indexing-rate.png)
  
  The Elasticsearch storage size is 117GiB on 3 nodes with a total of 12 shards without replication.
  
  The settings are slightly adapted: `index.refresh_interval` is set to 30s to reduce disk IO, `html_strip` is removed from the `fulltext` analyzer
  because of its high CPU cost and it brings nothing for text data. 
  Following is a flame graph showing that lots of time is spend in `CustomAnalyzer::initReader` that loop on the char filter set by `html_strip`: 
  
  ![html_strip_cost](https://s3-eu-west-1.amazonaws.com/nuxeo-b10b-results/160617-1739_3mongo-as-c3.2xlarge_1nuxeo-as-c3.2xlarge/b100m-indexing-html-strip.png)
  
  
## Gatling tests

  The first run reveals some regressions, fixes are in progress this step will be replayed soon. 
  
## Monitoring

  Some Grafana snapshots:
  
  - [import overview](https://snapshot.raintank.io/dashboard/snapshot/FOdRfauq5mwMiQIZ9jwIodhxuJb6WT3J)
  - [import details](https://snapshot.raintank.io/dashboard/snapshot/MYQJJRzT8uojxR7HoUi4SZRGLWxnEcH7)
  - [indexing overview](https://snapshot.raintank.io/dashboard/snapshot/ERRT3VX3LsIIypSCgDMhJvddMbojAItd?orgId=2)
  - [indexing details](https://snapshot.raintank.io/dashboard/snapshot/8iC2o53ofvJEvNjMGRtCAYsUo53k4NkT)
  - [Gatling overview]()
  - [Gatling details]()

